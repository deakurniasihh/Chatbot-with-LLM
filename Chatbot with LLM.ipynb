{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PsKQbqpLJzFqbGpZzcd-XA_XsuGti_NG","timestamp":1755513085846}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKDsuqqqZ6QW","executionInfo":{"status":"ok","timestamp":1755523293395,"user_tz":-420,"elapsed":32166,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"9153ee6b-dde8-4bf3-abf0-e418659707d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/download.py:141: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n","  warnings.warn(\n","\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n","Downloading 'zephyr-7b-beta.Q4_K_M.gguf' to '.cache/huggingface/download/-F7WFVRQSWxUCyEcmkHUpdzz3G8=.503580dce392c6e64669ad21a77023ba2a17baa0c381250fb67c11ba6406a85e.incomplete'\n","zephyr-7b-beta.Q4_K_M.gguf: 100% 4.37G/4.37G [00:27<00:00, 159MB/s]\n","Download complete. Moving file to zephyr-7b-beta.Q4_K_M.gguf\n","zephyr-7b-beta.Q4_K_M.gguf\n"]}],"source":["!huggingface-cli download TheBloke/zephyr-7B-beta-GGUF zephyr-7b-beta.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"]},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\""],"metadata":{"id":"GTGnl9ysjc9w","executionInfo":{"status":"ok","timestamp":1755523298718,"user_tz":-420,"elapsed":106,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install llama-cpp-python -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxloUYI3zz9y","executionInfo":{"status":"ok","timestamp":1755523542967,"user_tz":-420,"elapsed":241956,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"3218aa79-7ec3-4a4d-a4db-c96fa3c601bc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from llama_cpp import Llama\n","\n","\n","llm = Llama(\n","      model_path=\"/content/zephyr-7b-beta.Q4_K_M.gguf\",\n","      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n","      seed=1337, # Uncomment to set a specific seed\n","      n_ctx=2048, # Uncomment to increase the context window\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hYjQAcujd6F","executionInfo":{"status":"ok","timestamp":1755523985022,"user_tz":-420,"elapsed":9930,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"ed691144-66de-406f-d7fb-63ee1b0e2e38"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /content/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 15\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n","llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","print_info: file format = GGUF V3 (latest)\n","print_info: file type   = Q4_K - Medium\n","print_info: file size   = 4.07 GiB (4.83 BPW) \n","init_tokenizer: initializing tokenizer for type 1\n","load: control token:      2 '</s>' is not marked as EOG\n","load: control token:      1 '<s>' is not marked as EOG\n","load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","load: printing all EOG tokens:\n","load:   - 2 ('</s>')\n","load: special tokens cache size = 3\n","load: token to piece cache size = 0.1637 MB\n","print_info: arch             = llama\n","print_info: vocab_only       = 0\n","print_info: n_ctx_train      = 32768\n","print_info: n_embd           = 4096\n","print_info: n_layer          = 32\n","print_info: n_head           = 32\n","print_info: n_head_kv        = 8\n","print_info: n_rot            = 128\n","print_info: n_swa            = 0\n","print_info: is_swa_any       = 0\n","print_info: n_embd_head_k    = 128\n","print_info: n_embd_head_v    = 128\n","print_info: n_gqa            = 4\n","print_info: n_embd_k_gqa     = 1024\n","print_info: n_embd_v_gqa     = 1024\n","print_info: f_norm_eps       = 0.0e+00\n","print_info: f_norm_rms_eps   = 1.0e-05\n","print_info: f_clamp_kqv      = 0.0e+00\n","print_info: f_max_alibi_bias = 0.0e+00\n","print_info: f_logit_scale    = 0.0e+00\n","print_info: f_attn_scale     = 0.0e+00\n","print_info: n_ff             = 14336\n","print_info: n_expert         = 0\n","print_info: n_expert_used    = 0\n","print_info: causal attn      = 1\n","print_info: pooling type     = 0\n","print_info: rope type        = 0\n","print_info: rope scaling     = linear\n","print_info: freq_base_train  = 10000.0\n","print_info: freq_scale_train = 1\n","print_info: n_ctx_orig_yarn  = 32768\n","print_info: rope_finetuned   = unknown\n","print_info: model type       = 7B\n","print_info: model params     = 7.24 B\n","print_info: general.name     = huggingfaceh4_zephyr-7b-beta\n","print_info: vocab type       = SPM\n","print_info: n_vocab          = 32000\n","print_info: n_merges         = 0\n","print_info: BOS token        = 1 '<s>'\n","print_info: EOS token        = 2 '</s>'\n","print_info: UNK token        = 0 '<unk>'\n","print_info: PAD token        = 2 '</s>'\n","print_info: LF token         = 13 '<0x0A>'\n","print_info: EOG token        = 2 '</s>'\n","print_info: max token length = 48\n","load_tensors: loading model tensors, this can take a while... (mmap = true)\n","load_tensors: layer   0 assigned to device CPU, is_swa = 0\n","load_tensors: layer   1 assigned to device CPU, is_swa = 0\n","load_tensors: layer   2 assigned to device CPU, is_swa = 0\n","load_tensors: layer   3 assigned to device CPU, is_swa = 0\n","load_tensors: layer   4 assigned to device CPU, is_swa = 0\n","load_tensors: layer   5 assigned to device CPU, is_swa = 0\n","load_tensors: layer   6 assigned to device CPU, is_swa = 0\n","load_tensors: layer   7 assigned to device CPU, is_swa = 0\n","load_tensors: layer   8 assigned to device CPU, is_swa = 0\n","load_tensors: layer   9 assigned to device CPU, is_swa = 0\n","load_tensors: layer  10 assigned to device CPU, is_swa = 0\n","load_tensors: layer  11 assigned to device CPU, is_swa = 0\n","load_tensors: layer  12 assigned to device CPU, is_swa = 0\n","load_tensors: layer  13 assigned to device CPU, is_swa = 0\n","load_tensors: layer  14 assigned to device CPU, is_swa = 0\n","load_tensors: layer  15 assigned to device CPU, is_swa = 0\n","load_tensors: layer  16 assigned to device CPU, is_swa = 0\n","load_tensors: layer  17 assigned to device CPU, is_swa = 0\n","load_tensors: layer  18 assigned to device CPU, is_swa = 0\n","load_tensors: layer  19 assigned to device CPU, is_swa = 0\n","load_tensors: layer  20 assigned to device CPU, is_swa = 0\n","load_tensors: layer  21 assigned to device CPU, is_swa = 0\n","load_tensors: layer  22 assigned to device CPU, is_swa = 0\n","load_tensors: layer  23 assigned to device CPU, is_swa = 0\n","load_tensors: layer  24 assigned to device CPU, is_swa = 0\n","load_tensors: layer  25 assigned to device CPU, is_swa = 0\n","load_tensors: layer  26 assigned to device CPU, is_swa = 0\n","load_tensors: layer  27 assigned to device CPU, is_swa = 0\n","load_tensors: layer  28 assigned to device CPU, is_swa = 0\n","load_tensors: layer  29 assigned to device CPU, is_swa = 0\n","load_tensors: layer  30 assigned to device CPU, is_swa = 0\n","load_tensors: layer  31 assigned to device CPU, is_swa = 0\n","load_tensors: layer  32 assigned to device CPU, is_swa = 0\n","load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n","load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\n","load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n","repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",".repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n","...................\n","llama_context: constructing llama_context\n","llama_context: n_seq_max     = 1\n","llama_context: n_ctx         = 2048\n","llama_context: n_ctx_per_seq = 2048\n","llama_context: n_batch       = 512\n","llama_context: n_ubatch      = 512\n","llama_context: causal_attn   = 1\n","llama_context: flash_attn    = 0\n","llama_context: kv_unified    = false\n","llama_context: freq_base     = 10000.0\n","llama_context: freq_scale    = 1\n","llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n","set_abort_callback: call\n","llama_context:        CPU  output buffer size =     0.12 MiB\n","create_memory: n_ctx = 2048 (padded)\n","llama_kv_cache_unified: layer   0: dev = CPU\n","llama_kv_cache_unified: layer   1: dev = CPU\n","llama_kv_cache_unified: layer   2: dev = CPU\n","llama_kv_cache_unified: layer   3: dev = CPU\n","llama_kv_cache_unified: layer   4: dev = CPU\n","llama_kv_cache_unified: layer   5: dev = CPU\n","llama_kv_cache_unified: layer   6: dev = CPU\n","llama_kv_cache_unified: layer   7: dev = CPU\n","llama_kv_cache_unified: layer   8: dev = CPU\n","llama_kv_cache_unified: layer   9: dev = CPU\n","llama_kv_cache_unified: layer  10: dev = CPU\n","llama_kv_cache_unified: layer  11: dev = CPU\n","llama_kv_cache_unified: layer  12: dev = CPU\n","llama_kv_cache_unified: layer  13: dev = CPU\n","llama_kv_cache_unified: layer  14: dev = CPU\n","llama_kv_cache_unified: layer  15: dev = CPU\n","llama_kv_cache_unified: layer  16: dev = CPU\n","llama_kv_cache_unified: layer  17: dev = CPU\n","llama_kv_cache_unified: layer  18: dev = CPU\n","llama_kv_cache_unified: layer  19: dev = CPU\n","llama_kv_cache_unified: layer  20: dev = CPU\n","llama_kv_cache_unified: layer  21: dev = CPU\n","llama_kv_cache_unified: layer  22: dev = CPU\n","llama_kv_cache_unified: layer  23: dev = CPU\n","llama_kv_cache_unified: layer  24: dev = CPU\n","llama_kv_cache_unified: layer  25: dev = CPU\n","llama_kv_cache_unified: layer  26: dev = CPU\n","llama_kv_cache_unified: layer  27: dev = CPU\n","llama_kv_cache_unified: layer  28: dev = CPU\n","llama_kv_cache_unified: layer  29: dev = CPU\n","llama_kv_cache_unified: layer  30: dev = CPU\n","llama_kv_cache_unified: layer  31: dev = CPU\n","llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n","llama_kv_cache_unified: size =  256.00 MiB (  2048 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n","llama_context: enumerating backends\n","llama_context: backend_ptrs.size() = 1\n","llama_context: max_nodes = 2328\n","llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n","graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n","graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n","graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n","llama_context:        CPU compute buffer size =   168.01 MiB\n","llama_context: graph nodes  = 1126\n","llama_context: graph splits = 1\n","CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n","Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: llama-2\n"]}]},{"cell_type":"code","source":["llm(\"What is large Language Model?\", max_tokens=128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2v222pobkBZ1","executionInfo":{"status":"ok","timestamp":1755524068749,"user_tz":-420,"elapsed":76051,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"1b1de213-f1c6-44b1-dbfa-78cfbc065ece"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_perf_context_print:        load time =    2248.72 ms\n","llama_perf_context_print: prompt eval time =    2248.42 ms /     7 tokens (  321.20 ms per token,     3.11 tokens per second)\n","llama_perf_context_print:        eval time =   73681.60 ms /   127 runs   (  580.17 ms per token,     1.72 tokens per second)\n","llama_perf_context_print:       total time =   75998.36 ms /   134 tokens\n","llama_perf_context_print:    graphs reused =        122\n"]},{"output_type":"execute_result","data":{"text/plain":["{'id': 'cmpl-964d1dc9-8234-4f63-a793-5a50e5e47108',\n"," 'object': 'text_completion',\n"," 'created': 1755523992,\n"," 'model': '/content/zephyr-7b-beta.Q4_K_M.gguf',\n"," 'choices': [{'text': '\\n\\nIn a general sense, a language model is a statistical model that predicts the probability of a sequence of words. In a more specific sense, a language model is the core of a text generation system, such as Google’s DeepMind’s GPT (Generative Pre-trained Transformer) and OpenAI’s GPT-2. In this article, we will mainly focus on GPT.\\n\\nGPT is a powerful language model with over 1.5 billion parameters, trained on a vast corpus of text data. The model is capable of generating human-like text and can be fine',\n","   'index': 0,\n","   'logprobs': None,\n","   'finish_reason': 'length'}],\n"," 'usage': {'prompt_tokens': 7, 'completion_tokens': 128, 'total_tokens': 135}}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!pip install llama-index-llms-llama-cpp gradio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNOcxBh1kG8-","executionInfo":{"status":"ok","timestamp":1755524118882,"user_tz":-420,"elapsed":7817,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"2fd722e4-c2d8-4b46-cc61-d5173f1c3261"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: llama-index-llms-llama-cpp in /usr/local/lib/python3.11/dist-packages (0.5.0)\n","Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.42.0)\n","Requirement already satisfied: llama-cpp-python<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-llama-cpp) (0.3.16)\n","Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-llama-cpp) (0.13.2)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.10.0)\n","Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n","Requirement already satisfied: gradio-client==1.11.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.1)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n","Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.4)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n","Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.8)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio) (2025.3.0)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.1->gradio) (15.0.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.18.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.7)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (5.6.3)\n","Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (3.12.15)\n","Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.21.0)\n","Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (2.2.0)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.6.7)\n","Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.2.18)\n","Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.0.8)\n","Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n","Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.3.0)\n","Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (3.5)\n","Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (3.9.1)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (4.3.8)\n","Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (80.9.0)\n","Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (2.0.43)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (9.1.2)\n","Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.11.0)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.9.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.17.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.20.1)\n","Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.12.1)\n","Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.4.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (2024.11.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (3.2.4)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (1.1.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (3.26.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-llama-cpp) (0.4.6)\n"]}]},{"cell_type":"code","source":["from typing import Sequence, Optional\n","\n","from llama_index.core import (\n","    SimpleDirectoryReader,\n","    VectorStoreIndex,\n","    ServiceContext,\n",")\n","from llama_index.llms.llama_cpp import LlamaCPP\n","from llama_index.core.llms import ChatMessage\n","\n","\n","# definisi completion_to_prompt manual\n","def completion_to_prompt(completion: str) -> str:\n","    return f\"<|assistant|>\\n{completion}</s>\\n\"\n"],"metadata":{"id":"9I0-DFC6k36q","executionInfo":{"status":"ok","timestamp":1755524137782,"user_tz":-420,"elapsed":2936,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def zephyr_messages_to_prompt(\n","    messages: Sequence[ChatMessage],\n","    system_prompt: Optional[str] = None\n",") -> str:\n","    prompt = \"\"\n","    for message in messages:\n","        prompt += f\"<|{message.role}|>\\n\"\n","        prompt += f\"{message.content}</s>\\n\"\n","\n","    return prompt + \"<|assistant|>\\n\"\n"],"metadata":{"id":"RxUXS45XlxBh","executionInfo":{"status":"ok","timestamp":1755524141049,"user_tz":-420,"elapsed":6,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["llm = LlamaCPP(\n","    # You can pass in the URL to a GGML model to download it automatically\n","    model_url=None,\n","    # optionally, you can set the path to a pre-downloaded model instead of model_url\n","    model_path=\"/content/zephyr-7b-beta.Q4_K_M.gguf\",\n","    temperature=0.1,\n","    max_new_tokens=256,\n","    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n","    context_window=3900,\n","    # kwargs to pass to __call__()\n","    generate_kwargs={},\n","    # kwargs to pass to __init__()\n","    # set to at least 1 to use GPU\n","    model_kwargs={\"n_gpu_layers\": -1},\n","    # transform inputs into Llama2 format\n","    messages_to_prompt=zephyr_messages_to_prompt,\n","    completion_to_prompt=completion_to_prompt,\n","    verbose=True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vrY7xJGslJxu","executionInfo":{"status":"ok","timestamp":1755524147159,"user_tz":-420,"elapsed":4260,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"1324115c-8531-476a-b77d-818ccc63d285"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /content/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 15\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n","llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","print_info: file format = GGUF V3 (latest)\n","print_info: file type   = Q4_K - Medium\n","print_info: file size   = 4.07 GiB (4.83 BPW) \n","init_tokenizer: initializing tokenizer for type 1\n","load: control token:      2 '</s>' is not marked as EOG\n","load: control token:      1 '<s>' is not marked as EOG\n","load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","load: printing all EOG tokens:\n","load:   - 2 ('</s>')\n","load: special tokens cache size = 3\n","load: token to piece cache size = 0.1637 MB\n","print_info: arch             = llama\n","print_info: vocab_only       = 0\n","print_info: n_ctx_train      = 32768\n","print_info: n_embd           = 4096\n","print_info: n_layer          = 32\n","print_info: n_head           = 32\n","print_info: n_head_kv        = 8\n","print_info: n_rot            = 128\n","print_info: n_swa            = 0\n","print_info: is_swa_any       = 0\n","print_info: n_embd_head_k    = 128\n","print_info: n_embd_head_v    = 128\n","print_info: n_gqa            = 4\n","print_info: n_embd_k_gqa     = 1024\n","print_info: n_embd_v_gqa     = 1024\n","print_info: f_norm_eps       = 0.0e+00\n","print_info: f_norm_rms_eps   = 1.0e-05\n","print_info: f_clamp_kqv      = 0.0e+00\n","print_info: f_max_alibi_bias = 0.0e+00\n","print_info: f_logit_scale    = 0.0e+00\n","print_info: f_attn_scale     = 0.0e+00\n","print_info: n_ff             = 14336\n","print_info: n_expert         = 0\n","print_info: n_expert_used    = 0\n","print_info: causal attn      = 1\n","print_info: pooling type     = 0\n","print_info: rope type        = 0\n","print_info: rope scaling     = linear\n","print_info: freq_base_train  = 10000.0\n","print_info: freq_scale_train = 1\n","print_info: n_ctx_orig_yarn  = 32768\n","print_info: rope_finetuned   = unknown\n","print_info: model type       = 7B\n","print_info: model params     = 7.24 B\n","print_info: general.name     = huggingfaceh4_zephyr-7b-beta\n","print_info: vocab type       = SPM\n","print_info: n_vocab          = 32000\n","print_info: n_merges         = 0\n","print_info: BOS token        = 1 '<s>'\n","print_info: EOS token        = 2 '</s>'\n","print_info: UNK token        = 0 '<unk>'\n","print_info: PAD token        = 2 '</s>'\n","print_info: LF token         = 13 '<0x0A>'\n","print_info: EOG token        = 2 '</s>'\n","print_info: max token length = 48\n","load_tensors: loading model tensors, this can take a while... (mmap = true)\n","load_tensors: layer   0 assigned to device CPU, is_swa = 0\n","load_tensors: layer   1 assigned to device CPU, is_swa = 0\n","load_tensors: layer   2 assigned to device CPU, is_swa = 0\n","load_tensors: layer   3 assigned to device CPU, is_swa = 0\n","load_tensors: layer   4 assigned to device CPU, is_swa = 0\n","load_tensors: layer   5 assigned to device CPU, is_swa = 0\n","load_tensors: layer   6 assigned to device CPU, is_swa = 0\n","load_tensors: layer   7 assigned to device CPU, is_swa = 0\n","load_tensors: layer   8 assigned to device CPU, is_swa = 0\n","load_tensors: layer   9 assigned to device CPU, is_swa = 0\n","load_tensors: layer  10 assigned to device CPU, is_swa = 0\n","load_tensors: layer  11 assigned to device CPU, is_swa = 0\n","load_tensors: layer  12 assigned to device CPU, is_swa = 0\n","load_tensors: layer  13 assigned to device CPU, is_swa = 0\n","load_tensors: layer  14 assigned to device CPU, is_swa = 0\n","load_tensors: layer  15 assigned to device CPU, is_swa = 0\n","load_tensors: layer  16 assigned to device CPU, is_swa = 0\n","load_tensors: layer  17 assigned to device CPU, is_swa = 0\n","load_tensors: layer  18 assigned to device CPU, is_swa = 0\n","load_tensors: layer  19 assigned to device CPU, is_swa = 0\n","load_tensors: layer  20 assigned to device CPU, is_swa = 0\n","load_tensors: layer  21 assigned to device CPU, is_swa = 0\n","load_tensors: layer  22 assigned to device CPU, is_swa = 0\n","load_tensors: layer  23 assigned to device CPU, is_swa = 0\n","load_tensors: layer  24 assigned to device CPU, is_swa = 0\n","load_tensors: layer  25 assigned to device CPU, is_swa = 0\n","load_tensors: layer  26 assigned to device CPU, is_swa = 0\n","load_tensors: layer  27 assigned to device CPU, is_swa = 0\n","load_tensors: layer  28 assigned to device CPU, is_swa = 0\n","load_tensors: layer  29 assigned to device CPU, is_swa = 0\n","load_tensors: layer  30 assigned to device CPU, is_swa = 0\n","load_tensors: layer  31 assigned to device CPU, is_swa = 0\n","load_tensors: layer  32 assigned to device CPU, is_swa = 0\n","load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n","load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\n","load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n","repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",".repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n","repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n","repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n","repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n","repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n","repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n","repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n","repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n","...................\n","llama_context: constructing llama_context\n","llama_context: n_seq_max     = 1\n","llama_context: n_ctx         = 3900\n","llama_context: n_ctx_per_seq = 3900\n","llama_context: n_batch       = 512\n","llama_context: n_ubatch      = 512\n","llama_context: causal_attn   = 1\n","llama_context: flash_attn    = 0\n","llama_context: kv_unified    = false\n","llama_context: freq_base     = 10000.0\n","llama_context: freq_scale    = 1\n","llama_context: n_ctx_per_seq (3900) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n","set_abort_callback: call\n","llama_context:        CPU  output buffer size =     0.12 MiB\n","create_memory: n_ctx = 3904 (padded)\n","llama_kv_cache_unified: layer   0: dev = CPU\n","llama_kv_cache_unified: layer   1: dev = CPU\n","llama_kv_cache_unified: layer   2: dev = CPU\n","llama_kv_cache_unified: layer   3: dev = CPU\n","llama_kv_cache_unified: layer   4: dev = CPU\n","llama_kv_cache_unified: layer   5: dev = CPU\n","llama_kv_cache_unified: layer   6: dev = CPU\n","llama_kv_cache_unified: layer   7: dev = CPU\n","llama_kv_cache_unified: layer   8: dev = CPU\n","llama_kv_cache_unified: layer   9: dev = CPU\n","llama_kv_cache_unified: layer  10: dev = CPU\n","llama_kv_cache_unified: layer  11: dev = CPU\n","llama_kv_cache_unified: layer  12: dev = CPU\n","llama_kv_cache_unified: layer  13: dev = CPU\n","llama_kv_cache_unified: layer  14: dev = CPU\n","llama_kv_cache_unified: layer  15: dev = CPU\n","llama_kv_cache_unified: layer  16: dev = CPU\n","llama_kv_cache_unified: layer  17: dev = CPU\n","llama_kv_cache_unified: layer  18: dev = CPU\n","llama_kv_cache_unified: layer  19: dev = CPU\n","llama_kv_cache_unified: layer  20: dev = CPU\n","llama_kv_cache_unified: layer  21: dev = CPU\n","llama_kv_cache_unified: layer  22: dev = CPU\n","llama_kv_cache_unified: layer  23: dev = CPU\n","llama_kv_cache_unified: layer  24: dev = CPU\n","llama_kv_cache_unified: layer  25: dev = CPU\n","llama_kv_cache_unified: layer  26: dev = CPU\n","llama_kv_cache_unified: layer  27: dev = CPU\n","llama_kv_cache_unified: layer  28: dev = CPU\n","llama_kv_cache_unified: layer  29: dev = CPU\n","llama_kv_cache_unified: layer  30: dev = CPU\n","llama_kv_cache_unified: layer  31: dev = CPU\n","llama_kv_cache_unified:        CPU KV buffer size =   488.00 MiB\n","llama_kv_cache_unified: size =  488.00 MiB (  3904 cells,  32 layers,  1/1 seqs), K (f16):  244.00 MiB, V (f16):  244.00 MiB\n","llama_context: enumerating backends\n","llama_context: backend_ptrs.size() = 1\n","llama_context: max_nodes = 2328\n","llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n","graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n","graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n","graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n","llama_context:        CPU compute buffer size =   287.63 MiB\n","llama_context: graph nodes  = 1126\n","llama_context: graph splits = 1\n","CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n","Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: llama-2\n"]}]},{"cell_type":"code","source":["messages = [\n","    ChatMessage(\n","        role=\"system\",\n","        content=\"You are a chatbot who always responds concise and pleasant\"\n","    ),\n","    ChatMessage(\n","        role=\"user\",\n","        content=\"There's a llama on my lawn, how can I get rid of him?\"\n","    )\n","]\n","\n","# zephyr_messages_to_prompt(messages)\n","\n","response = llm.stream_chat(messages)\n","for r in response:\n","  print(r.delta, end=\"\", flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5Vz-_wRmyl0","executionInfo":{"status":"ok","timestamp":1755516864331,"user_tz":-420,"elapsed":151630,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"6931623e-f4b0-42bb-acdb-4757df4e251d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["I'm sorry to hear that. Here are a few suggestions to help you get rid of the llama:\n","\n","1. Contact your local animal control agency or wildlife rehabilitation center for assistance. They may be able to provide guidance on how to safely and humanely remove the llama from your property.\n","\n","2. Try to lure the llama away with food or water. This may help convince it to leave on its own.\n","\n","3. Use a loud noise or bright light to startle the llama and encourage it to move along.\n","\n","4. If the llama is causing damage to your property, you may need to consider hiring a professional llama wrangler or animal removal service.\n","\n","5. In the future, consider installing a fence or other barrier to prevent llamas and other animals from entering your property.\n","\n","Remember to always prioritize the safety and well-being of the animal, as well as your own safety."]},{"output_type":"stream","name":"stderr","text":["llama_perf_context_print:        load time =   15820.24 ms\n","llama_perf_context_print: prompt eval time =   15819.86 ms /    63 tokens (  251.11 ms per token,     3.98 tokens per second)\n","llama_perf_context_print:        eval time =  135317.66 ms /   202 runs   (  669.89 ms per token,     1.49 tokens per second)\n","llama_perf_context_print:       total time =  151567.82 ms /   265 tokens\n","llama_perf_context_print:    graphs reused =        194\n"]}]},{"cell_type":"code","source":["import gradio as gr\n","\n","\n","def generate_response(message, history):\n","    chat_messages = [\n","        ChatMessage(\n","            role=\"system\",\n","            content=\"You are a chatbot who always responds concise and pleasant\"\n","        ),\n","    ]\n","    for human, ai in history:\n","        chat_messages.append(ChatMessage(\n","            role=\"user\",\n","            content=human\n","        ))\n","        chat_messages.append(ChatMessage(\n","            role=\"assistant\",\n","            content=ai\n","        ))\n","\n","    chat_messages.append(ChatMessage(\n","        role=\"user\",\n","        content=message\n","    ))\n","\n","    response = llm.stream_chat(chat_messages)\n","    text = \"\"\n","    for r in response:\n","      text += r.delta\n","      yield text\n","\n","gr.ChatInterface(generate_response).launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"id":"inCbYDnlnawd","executionInfo":{"status":"ok","timestamp":1755524152583,"user_tz":-420,"elapsed":5019,"user":{"displayName":"152021165 DEA KURNIASIH","userId":"12234987222967703539"}},"outputId":"a21ee9c5-5151-45ab-adc1-f1e5d56c4455"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://fb75c1bf0ffc9406dd.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://fb75c1bf0ffc9406dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]}]}